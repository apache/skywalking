# The remote server should connect to, hostname can be either hostname or IP address.
# Suggestion: set the real ip address.
cluster.current.hostname=127.0.0.1
cluster.current.port=11800

# The roles of this member. List of strings, e.g. roles = A, B
# In the future, the roles are part of the membership information and can be used by
# routers or other services to distribute work to certain member types,
# e.g. front-end and back-end nodes.
# In this version, all members has same roles, each of them will listen others status,
# because of network trouble or member jvm crash or every reason led to not reachable,
# the routers will stop to sending the message to the untouchable member.
cluster.current.roles=WorkersListener

# Initial contact points of the cluster, e.g. seed_nodes = 127.0.0.1:11800, 127.0.0.1:11801.
# The nodes to join automatically at startup.
# When setting akka configuration, it will be change.
# like: ["akka.tcp://system@127.0.0.1:11800", "akka.tcp://system@127.0.0.1:11801"].
# This is akka configuration, see: http://doc.akka.io/docs/akka/2.4/general/configuration.html
cluster.seed_nodes=127.0.0.1:11800

# elasticsearch configuration, config/elasticsearch.yml, see cluster.name
es.cluster.name=CollectorDBCluster
es.cluster.transport.sniffer=true

# The elasticsearch nodes of cluster, comma separated, e.g. nodes=ip:port, ip:port
es.cluster.nodes=127.0.0.1:9300

# Automatic create elasticsearch index
# Options: auto, forced, manual
# auto: just create new index when index not created.
# forced: delete the index then create
es.index.initialize.mode=auto
es.index.shards.number=2
es.index.replicas.number=0

# You can configure a host either as a host name or IP address to identify a specific network
# interface on which to listen.
# Be used for web ui get the view data or agent post the trace segment.
http.hostname=127.0.0.1
# The TCP/IP port on which the connector listens for connections.
http.port=12800
# The contextPath is a URL prefix that identifies which context a HTTP request is destined for.
http.contextPath=/

# The analysis worker max cache size, when worker data size reach the size,
# then worker will send all cached data to the next worker and clear the cache.
cache.analysis.size=1024
# The persistence worker max cache size, same of "cache.analysis.size" ability.
cache.persistence.size=1024

WorkerNum.Node.NodeCompAgg.Value=10
WorkerNum.Node.NodeMappingDayAgg.Value=10
WorkerNum.Node.NodeMappingHourAgg.Value=10
WorkerNum.Node.NodeMappingMinuteAgg.Value=10

WorkerNum.NodeRef.NodeRefDayAgg.Value=10
WorkerNum.NodeRef.NodeRefHourAgg.Value=10
WorkerNum.NodeRef.NodeRefMinuteAgg.Value=10
WorkerNum.NodeRef.NodeRefResSumDayAgg.Value=10
WorkerNum.NodeRef.NodeRefResSumHourAgg.Value=10
WorkerNum.NodeRef.NodeRefResSumMinuteAgg.Value=10

WorkerNum.GlobalTrace.GlobalTraceAgg.Value=10

Queue.GlobalTrace.GlobalTraceSave.Size=1024
Queue.GlobalTrace.GlobalTraceAnalysis.Size=1024

Queue.Segment.SegmentPost.Size=1024
Queue.Segment.SegmentCostSave.Size=1024
Queue.Segment.SegmentSave.Size=1024
Queue.Segment.SegmentExceptionSave.Size=1024

Queue.Node.NodeCompAnalysis.Size=1024
Queue.Node.NodeMappingDayAnalysis.Size=1024
Queue.Node.NodeMappingHourAnalysis.Size=1024
Queue.Node.NodeMappingMinuteAnalysis.Size=1024
Queue.Node.NodeCompSave.Size=1024
Queue.Node.NodeMappingDaySave.Size=1024
Queue.Node.NodeMappingHourSave.Size=1024
Queue.Node.NodeMappingMinuteSave.Size=1024

Queue.NodeRef.NodeRefDayAnalysis.Size=1024
Queue.NodeRef.NodeRefHourAnalysis.Size=1024
Queue.NodeRef.NodeRefMinuteAnalysis.Size=1024
Queue.NodeRef.NodeRefDaySave.Size=1024
Queue.NodeRef.NodeRefHourSave.Size=1024
Queue.NodeRef.NodeRefMinuteSave.Size=1024
Queue.NodeRef.NodeRefResSumDaySave.Size=1024
Queue.NodeRef.NodeRefResSumHourSave.Size=1024
Queue.NodeRef.NodeRefResSumMinuteSave.Size=1024
Queue.NodeRef.NodeRefResSumDayAnalysis.Size=1024
Queue.NodeRef.NodeRefResSumHourAnalysis.Size=1024
Queue.NodeRef.NodeRefResSumMinuteAnalysis.Size=1024

