# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This will parse a textual representation of a duration. The formats
# accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS}
# with days considered to be exactly 24 hours.
# <p>
# Examples:
# <pre>
#    "PT20.345S" -- parses as "20.345 seconds"
#    "PT15M"     -- parses as "15 minutes" (where a minute is 60 seconds)
#    "PT10H"     -- parses as "10 hours" (where an hour is 3600 seconds)
#    "P2D"       -- parses as "2 days" (where a day is 24 hours or 86400 seconds)
#    "P2DT3H4M"  -- parses as "2 days, 3 hours and 4 minutes"
#    "P-6H3M"    -- parses as "-6 hours and +3 minutes"
#    "-P6H3M"    -- parses as "-6 hours and -3 minutes"
#    "-P-6H+3M"  -- parses as "+6 hours and -3 minutes"
# </pre>
filter: "{ tags -> tags.job_name == 'bookkeeper-monitoring' }" # The OpenTelemetry job name
expSuffix: tag({tags -> tags.cluster = 'bookkeeper::' + tags.cluster}).instance(['cluster'], ['node'], Layer.BOOKKEEPER)
metricPrefix: meter_bookkeeper_node

# Metrics Rules
metricsRules:
  # thread executor metrics
  - name: thread_executor_completed
    exp: bookkeeper_server_thread_executor_completed.sum(['cluster', 'node'])
  - name: thread_executor_tasks_completed
    exp: bookkeeper_server_thread_executor_tasks_completed.sum(['cluster', 'node'])
  - name: thread_executor_tasks_rejected
    exp: bookkeeper_server_thread_executor_tasks_rejected.sum(['cluster', 'node'])
  - name: thread_executor_tasks_failed
    exp: bookkeeper_server_thread_executor_tasks_failed.sum(['cluster', 'node'])

  - name: high_priority_threads
    exp: bookkeeper_server_BookieHighPriorityThread_threads.sum(['cluster', 'node'])
  - name: read_thread_pool_threads
    exp: bookkeeper_server_BookieReadThreadPool_threads.sum(['cluster', 'node'])
  - name: high_priority_thread_max_queue_size
    exp: bookkeeper_server_BookieHighPriorityThread_max_queue_size.sum(['cluster', 'node'])
  - name: read_thread_pool_max_queue_size
    exp: bookkeeper_server_BookieReadThreadPool_max_queue_size.sum(['cluster', 'node'])

  # JVM Metrics
  - name: jvm_memory_used
    exp: jvm_memory_bytes_used.sum(['cluster', 'node'])
  - name: jvm_memory_committed
    exp: jvm_memory_bytes_committed.sum(['cluster', 'node'])
  - name: jvm_memory_init
    exp: jvm_memory_bytes_init.sum(['cluster', 'node'])

  - name: jvm_memory_pool_used
    exp: jvm_memory_pool_bytes_used.sum(['cluster', 'node', 'pool'])
  - name: jvm_memory_pool_committed
    exp: jvm_memory_pool_bytes_committed.sum(['cluster', 'node', 'pool'])
  - name: jvm_memory_pool_init
    exp: jvm_memory_pool_bytes_init.sum(['cluster', 'node', 'pool'])

  - name: jvm_buffer_pool_used_bytes
    exp: jvm_buffer_pool_used_bytes.sum(['cluster', 'node', 'pool'])
  - name: jvm_buffer_pool_capacity_bytes
    exp: jvm_buffer_pool_capacity_bytes.sum(['cluster', 'node', 'pool'])
  - name: jvm_buffer_pool_used_buffers
    exp: jvm_buffer_pool_used_buffers.sum(['cluster', 'node', 'pool'])

  - name: jvm_gc_collection_seconds_count
    exp: jvm_gc_collection_seconds_count.sum(['cluster', 'node', 'gc']).rate('PT1M')
  - name: jvm_gc_collection_seconds_sum
    exp: jvm_gc_collection_seconds_sum.sum(['cluster', 'node', 'gc']).rate('PT1M')

  - name: jvm_threads_current
    exp: jvm_threads_current.sum(['cluster', 'node'])
  - name: jvm_threads_daemon
    exp: jvm_threads_daemon.sum(['cluster', 'node'])
  - name: jvm_threads_peak
    exp: jvm_threads_peak.sum(['cluster', 'node'])
  - name: jvm_threads_deadlocked
    exp: jvm_threads_deadlocked.sum(['cluster', 'node'])
